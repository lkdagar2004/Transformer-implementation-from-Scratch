{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.761500Z","iopub.execute_input":"2026-02-17T10:37:02.761857Z","iopub.status.idle":"2026-02-17T10:37:02.767430Z","shell.execute_reply.started":"2026-02-17T10:37:02.761828Z","shell.execute_reply":"2026-02-17T10:37:02.766099Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ==========================================\n# 1. DATA PREPARATION (Toy Dataset)\n# ==========================================\n# For demonstration, we use a tiny hardcoded dataset.\n# In a real project, you would load thousands of sentences from a file.\nraw_data = [\n    (\"hello\", \"hola\"),\n    (\"good morning\", \"buenos dias\"),\n    (\"how are you\", \"como estas\"),\n    (\"i am fine\", \"estoy bien\"),\n    (\"see you later\", \"hasta luego\"),\n    (\"thank you\", \"gracias\"),\n    (\"what is your name\", \"como te llamas\"),\n    (\"my name is ai\", \"me llamo ia\"),\n    (\"goodbye\", \"adios\"),\n    (\"have a nice day\", \"que tengas un buen dia\")\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.781287Z","iopub.execute_input":"2026-02-17T10:37:02.781593Z","iopub.status.idle":"2026-02-17T10:37:02.789577Z","shell.execute_reply.started":"2026-02-17T10:37:02.781569Z","shell.execute_reply":"2026-02-17T10:37:02.788181Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Simple Tokenizer (Splitting by space)\n# We build a vocabulary from the raw data\ndef build_vocab(sentences):\n    vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n    idx = 4\n    for sent in sentences:\n        for word in sent.split():\n            if word not in vocab:\n                vocab[word] = idx\n                idx += 1\n    return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.791529Z","iopub.execute_input":"2026-02-17T10:37:02.792157Z","iopub.status.idle":"2026-02-17T10:37:02.816533Z","shell.execute_reply.started":"2026-02-17T10:37:02.792109Z","shell.execute_reply":"2026-02-17T10:37:02.815145Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Create Vocabularies\nsrc_sentences, tgt_sentences = zip(*raw_data)\nsrc_vocab = build_vocab(src_sentences)\ntgt_vocab = build_vocab(tgt_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.818662Z","iopub.execute_input":"2026-02-17T10:37:02.819639Z","iopub.status.idle":"2026-02-17T10:37:02.839231Z","shell.execute_reply.started":"2026-02-17T10:37:02.819596Z","shell.execute_reply":"2026-02-17T10:37:02.837532Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Reverse lookup (Index to Word) for decoding later\nidx2word_tgt = {v: k for k, v in tgt_vocab.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.841262Z","iopub.execute_input":"2026-02-17T10:37:02.841672Z","iopub.status.idle":"2026-02-17T10:37:02.859536Z","shell.execute_reply.started":"2026-02-17T10:37:02.841602Z","shell.execute_reply":"2026-02-17T10:37:02.857978Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Helper to convert sentence to tensor\ndef sentence_to_tensor(sentence, vocab):\n    tokens = [vocab.get(word, vocab['<unk>']) for word in sentence.split()]\n    tokens = [vocab['<start>']] + tokens + [vocab['<end>']]\n    # Pad to fixed length for batching (simplified here)\n    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0) # Batch size 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.861783Z","iopub.execute_input":"2026-02-17T10:37:02.862250Z","iopub.status.idle":"2026-02-17T10:37:02.880121Z","shell.execute_reply.started":"2026-02-17T10:37:02.862220Z","shell.execute_reply":"2026-02-17T10:37:02.878858Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = torch.softmax(scores, dim=-1)\n        return torch.matmul(attn, V)\n\n    def split_heads(self, x):\n        batch_size, seq_len, _ = x.size()\n        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        \n    def combine_heads(self, x):\n        batch_size, _, seq_len, _ = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n    \n    # !!! MAKE SURE THIS LINE IS ALIGNED WITH def __init__ !!!\n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.882023Z","iopub.execute_input":"2026-02-17T10:37:02.883147Z","iopub.status.idle":"2026-02-17T10:37:02.903085Z","shell.execute_reply.started":"2026-02-17T10:37:02.883025Z","shell.execute_reply":"2026-02-17T10:37:02.901602Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.961673Z","iopub.execute_input":"2026-02-17T10:37:02.962706Z","iopub.status.idle":"2026-02-17T10:37:02.969347Z","shell.execute_reply.started":"2026-02-17T10:37:02.962667Z","shell.execute_reply":"2026-02-17T10:37:02.968160Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.971139Z","iopub.execute_input":"2026-02-17T10:37:02.971489Z","iopub.status.idle":"2026-02-17T10:37:02.993419Z","shell.execute_reply.started":"2026-02-17T10:37:02.971434Z","shell.execute_reply":"2026-02-17T10:37:02.992139Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, mask)))\n        x = self.norm2(x + self.dropout(self.feed_forward(x)))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:02.995361Z","iopub.execute_input":"2026-02-17T10:37:02.995771Z","iopub.status.idle":"2026-02-17T10:37:03.023072Z","shell.execute_reply.started":"2026-02-17T10:37:02.995731Z","shell.execute_reply":"2026-02-17T10:37:03.021847Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)))\n        x = self.norm3(x + self.dropout(self.feed_forward(x)))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:03.024621Z","iopub.execute_input":"2026-02-17T10:37:03.025089Z","iopub.status.idle":"2026-02-17T10:37:03.045507Z","shell.execute_reply.started":"2026-02-17T10:37:03.025018Z","shell.execute_reply":"2026-02-17T10:37:03.044023Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_len = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask.to(tgt.device)\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        enc_output = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n            \n        dec_output = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n            \n        return self.fc(dec_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:03.047642Z","iopub.execute_input":"2026-02-17T10:37:03.048030Z","iopub.status.idle":"2026-02-17T10:37:03.067321Z","shell.execute_reply.started":"2026-02-17T10:37:03.048002Z","shell.execute_reply":"2026-02-17T10:37:03.066163Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# ==========================================\n# 3. CONFIGURATION & TRAINING\n# ==========================================\n\n# Hyperparameters\nd_model = 128      # Reduced for small example\nnum_heads = 4\nnum_layers = 2\nd_ff = 256\nmax_len = 20\ndropout = 0.1\nlearning_rate = 0.001\nepochs = 50\n\n# Initialize Model\nmodel = Transformer(len(src_vocab), len(tgt_vocab), d_model, num_heads, num_layers, d_ff, max_len, dropout)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nprint(\"Starting Training...\")\nmodel.train()\n\nfor epoch in range(epochs):\n    total_loss = 0\n    for src_text, tgt_text in raw_data:\n        # Prepare inputs\n        src = sentence_to_tensor(src_text, src_vocab)\n        tgt = sentence_to_tensor(tgt_text, tgt_vocab)\n        \n        # Shift target for training (Input: <start> A B, Output: A B <end>)\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n        \n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n        \n        # Reshape for loss calculation\n        loss = criterion(output.contiguous().view(-1, len(tgt_vocab)), tgt_output.contiguous().view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    if (epoch+1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(raw_data):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:37:03.068574Z","iopub.execute_input":"2026-02-17T10:37:03.069275Z","iopub.status.idle":"2026-02-17T10:37:13.899766Z","shell.execute_reply.started":"2026-02-17T10:37:03.069228Z","shell.execute_reply":"2026-02-17T10:37:13.898845Z"}},"outputs":[{"name":"stdout","text":"Starting Training...\nEpoch 10/50, Loss: 0.0664\nEpoch 20/50, Loss: 0.0221\nEpoch 30/50, Loss: 0.0135\nEpoch 40/50, Loss: 0.0079\nEpoch 50/50, Loss: 0.0060\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# ==========================================\n# 4. INFERENCE (TRANSLATION)\n# ==========================================\n\ndef translate(sentence):\n    model.eval()\n    src = sentence_to_tensor(sentence, src_vocab)\n    tgt_input = torch.tensor([[tgt_vocab['<start>']]], dtype=torch.long)\n    \n    # Greedy decoding loop\n    for _ in range(max_len):\n        with torch.no_grad():\n            output = model(src, tgt_input)\n            # Get the next word token (last position)\n            next_token = output.argmax(dim=-1)[:, -1].item()\n            \n            # Stop if end token is reached\n            if next_token == tgt_vocab['<end>']:\n                break\n                \n            # Append to input for next iteration\n            tgt_input = torch.cat([tgt_input, torch.tensor([[next_token]] )], dim=1)\n    \n    # Convert tokens back to words\n    decoded_words = [idx2word_tgt[idx.item()] for idx in tgt_input[0][1:]]\n    return \" \".join(decoded_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:38:14.442815Z","iopub.execute_input":"2026-02-17T10:38:14.443251Z","iopub.status.idle":"2026-02-17T10:38:14.451882Z","shell.execute_reply.started":"2026-02-17T10:38:14.443221Z","shell.execute_reply":"2026-02-17T10:38:14.450670Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# ==========================================\n# 5. TESTING\n# ==========================================\nprint(\"\\n--- Translation Results ---\")\ntest_sentence = \"how are you\"\nprint(f\"English: {test_sentence}\")\nprint(f\"Spanish (Model): {translate(test_sentence)}\")\n\ntest_sentence_2 = \"good morning\"\nprint(f\"English: {test_sentence_2}\")\nprint(f\"Spanish (Model): {translate(test_sentence_2)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:38:21.531433Z","iopub.execute_input":"2026-02-17T10:38:21.532345Z","iopub.status.idle":"2026-02-17T10:38:21.571708Z","shell.execute_reply.started":"2026-02-17T10:38:21.532312Z","shell.execute_reply":"2026-02-17T10:38:21.570345Z"}},"outputs":[{"name":"stdout","text":"\n--- Translation Results ---\nEnglish: how are you\nSpanish (Model): como estas\nEnglish: good morning\nSpanish (Model): buenos dias\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# You might need to install nltk first: pip install nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:42:40.362733Z","iopub.execute_input":"2026-02-17T10:42:40.363664Z","iopub.status.idle":"2026-02-17T10:42:42.485123Z","shell.execute_reply.started":"2026-02-17T10:42:40.363631Z","shell.execute_reply":"2026-02-17T10:42:42.484135Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def calculate_bleu_score(model, dataset):\n    model.eval()\n    total_bleu = 0\n    \n    # Smoothing is needed for short sentences/small datasets to avoid \"0.0\" scores\n    smoothie = SmoothingFunction().method1\n    \n    with torch.no_grad():\n        for src_text, tgt_text in dataset:\n            # 1. Translate the source sentence\n            prediction = translate(src_text)\n            \n            # 2. Prepare for BLEU (needs tokenized lists)\n            # Reference: List of acceptable translations (we have only 1 per sentence)\n            reference = [tgt_text.split()] \n            \n            # Candidate: The model's prediction\n            candidate = prediction.split()\n            \n            # 3. Calculate Score for this sentence\n            score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n            total_bleu += score\n            \n            # Optional: Print details to see what's happening\n            # print(f\"Src: {src_text}\")\n            # print(f\"Ref: {tgt_text}\")\n            # print(f\"Pred: {prediction}\")\n            # print(f\"Score: {score:.4f}\\n\")\n\n    # Average BLEU across the dataset\n    avg_bleu = total_bleu / len(dataset)\n    return avg_bleu * 100  # Return as percentage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:42:49.476544Z","iopub.execute_input":"2026-02-17T10:42:49.477549Z","iopub.status.idle":"2026-02-17T10:42:49.484521Z","shell.execute_reply.started":"2026-02-17T10:42:49.477512Z","shell.execute_reply":"2026-02-17T10:42:49.483348Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"print(f\"Final BLEU Score: {calculate_bleu_score(model, raw_data):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:42:56.021941Z","iopub.execute_input":"2026-02-17T10:42:56.022274Z","iopub.status.idle":"2026-02-17T10:42:56.151541Z","shell.execute_reply.started":"2026-02-17T10:42:56.022241Z","shell.execute_reply":"2026-02-17T10:42:56.150231Z"}},"outputs":[{"name":"stdout","text":"Final BLEU Score: 39.23\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}